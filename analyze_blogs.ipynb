{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim import corpora, models\n",
    "import spacy\n",
    "from spacy.lang.en import stop_words\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/linguistic-features\n",
    "\n",
    "\n",
    "def preprocess_text(file_path, nlp, stop_word_removal=True, non_alpha_removal=True, lemmatization=True, lowercasing=True, additional_stop_words=[]):\n",
    "    stop_words_to_use = list(stop_words.STOP_WORDS)\n",
    "    stop_words_to_use.append(additional_stop_words)\n",
    "\n",
    "    documents = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for blog in tqdm(f):\n",
    "            doc = nlp(blog.strip()) # Convert to spaCy doc\n",
    "\n",
    "            if len(doc) > 1:\n",
    "                if non_alpha_removal: # Remove non alpha characters\n",
    "                    doc = [token for token in doc if token.is_alpha]\n",
    "                \n",
    "                if lemmatization: # Lemmatize words\n",
    "                    doc = [token.lemma_ for token in doc]\n",
    "\n",
    "                if stop_word_removal: # Remove stop words\n",
    "                    doc = [token for token in doc if token not in stop_words_to_use]                \n",
    "                \n",
    "                if lowercasing: # Lowercase words\n",
    "                    doc = [token.lower() for token in doc]\n",
    "\n",
    "                documents.append(doc)\n",
    "\n",
    "    # https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html\n",
    "    dictionary = corpora.Dictionary(documents)\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "    dictionary.save(\"dictionary2.dict\")\n",
    "    corpora.MmCorpus.serialize(\"corpus2.mm\", corpus)\n",
    "\n",
    "preprocess_text(\"blogs.txt\", nlp, additional_stop_words=[\"ride\", \"day\", \"bike\", \"road\", \"get\", \"go\", \"mile\", \"like\", \"way\", \"good\", \"come\", \"look\", \"nice\", \"think\", \"trip\", \"know\", \"see\", \"great\", \"today\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html\n",
    "\n",
    "# dictionary = corpora.Dictionary.load(\"dictionary.dict\")\n",
    "# corpus = corpora.MmCorpus(\"corpus.mm\")\n",
    "\n",
    "# word_freq = {k: v for k, v in sorted(dictionary.cfs.items(), key = lambda item: item[1], reverse=True)}\n",
    "# for id in list(word_freq.keys())[:100]:\n",
    "#     print(dictionary[id])\n",
    "\n",
    "# #%% Modelling\n",
    "\n",
    "# tfidf = models.TfidfModel(corpus)\n",
    "# corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "# # # https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "# lda = models.LdaModel(corpus_tfidf, num_topics=10)\n",
    "\n",
    "# topics = lda.get_topics()\n",
    "# for topic in range(10):\n",
    "#     topic_probs = topics[topic, :]\n",
    "#     print(f\"Topic {topic}: {\", \".join([dictionary[i] for i in np.argsort(topic_probs)[-10:]])}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
