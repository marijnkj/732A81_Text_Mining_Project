{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfeeK6JJ5OXt"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "BFsqgr8W5OXw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim import corpora, models\n",
        "import spacy\n",
        "from spacy.lang.en import stop_words\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "# !python -m spacy download en_core_web_md\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fzVmVrx5OXx"
      },
      "source": [
        "## Preprocess text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter out non-English documents"
      ],
      "metadata": {
        "id": "ZcoIJ2rExoNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/papluca/xlm-roberta-base-language-detection\n",
        "model_ckpt = \"papluca/xlm-roberta-base-language-detection\"\n",
        "pipe = pipeline(\"text-classification\", model=model_ckpt)\n",
        "\n",
        "df_blogs = pd.DataFrame(columns=[\"blog\", \"lang\"])\n",
        "with open(\"blogs.txt\", \"r\", encoding=\"latin\") as f:\n",
        "  i = 0\n",
        "  for blog in tqdm(f):\n",
        "    blog = blog.strip()\n",
        "    lang = pipe(blog, truncation=True)\n",
        "\n",
        "    df_blogs.loc[i, \"blog\"] = blog\n",
        "    df_blogs.loc[i, \"lang\"] = lang[0][\"label\"]\n",
        "\n",
        "    i += 1\n",
        "\n",
        "df_blogs = df_blogs[df_blogs[\"lang\"] == \"en\"]\n",
        "df_blogs.to_csv(\"blogs.csv\")"
      ],
      "metadata": {
        "id": "axOe9Lm1wYOc",
        "outputId": "88d6de38-d40f-42e1-b597-64c93497f786",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply standard preprocessing"
      ],
      "metadata": {
        "id": "v0w8f8f_xtQd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "d4ismve05OXy",
        "outputId": "72f431e9-fe00-4e4d-aa43-16fbd9d1a742",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "290it [00:28,  7.81it/s]"
          ]
        }
      ],
      "source": [
        "# https://spacy.io/usage/linguistic-features\n",
        "\n",
        "\n",
        "def preprocess_text(file_path, nlp, stop_word_removal=True, non_alpha_removal=True, lemmatization=True, lowercasing=True, additional_stop_words=[]):\n",
        "    stop_words_to_use = list(stop_words.STOP_WORDS)\n",
        "    stop_words_to_use += additional_stop_words\n",
        "\n",
        "    df_blogs = pd.read_csv(file_path)\n",
        "    documents = []\n",
        "\n",
        "    for i, row in tqdm(df_blogs.iterrows()):\n",
        "        doc = nlp(row[\"blog\"]) # Convert to spaCy doc\n",
        "\n",
        "        if len(doc) > 1:\n",
        "            if non_alpha_removal: # Remove non alpha characters\n",
        "                doc = [token for token in doc if token.is_alpha]\n",
        "\n",
        "            if lemmatization: # Lemmatize words\n",
        "                doc = [token.lemma_ for token in doc]\n",
        "\n",
        "            if stop_word_removal: # Remove stop words\n",
        "                doc = [token for token in doc if token not in stop_words_to_use]\n",
        "\n",
        "            if lowercasing: # Lowercase words\n",
        "                doc = [token.lower() for token in doc]\n",
        "\n",
        "            documents.append(doc)\n",
        "\n",
        "    # https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html\n",
        "    dictionary = corpora.Dictionary(documents)\n",
        "    dictionary.filter_extremes(no_below=2) # Filter out tokens appearing only once\n",
        "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
        "\n",
        "    dictionary.save(\"dictionary.dict\")\n",
        "    corpora.MmCorpus.serialize(\"corpus.mm\", corpus)\n",
        "\n",
        "additional_stop_words = [\"ride\", \"day\", \"bike\", \"road\", \"get\", \"go\", \"mile\", \"km\", \"metre\", \"like\", \"way\", \"good\", \"come\", \"look\", \"nice\", \"think\", \"trip\", \"know\", \"see\", \"great\", \"today\"]\n",
        "preprocess_text(\"blogs.csv\", nlp, additional_stop_words=additional_stop_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore dictionary"
      ],
      "metadata": {
        "id": "Dfn7rYdO8N5J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "echHEMnu5OXz",
        "outputId": "3e0b65f0-45db-4c73-caa1-8a98b3507491",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i\n",
            "time\n",
            "stop\n",
            "town\n",
            "find\n",
            "little\n",
            "head\n",
            "hill\n",
            "start\n",
            "pass\n",
            "night\n",
            "turn\n",
            "leave\n",
            "place\n",
            "park\n",
            "long\n",
            "lot\n",
            "people\n",
            "hour\n",
            "climb\n",
            "rain\n",
            "feel\n",
            "big\n",
            "right\n",
            "eat\n",
            "want\n",
            "wind\n",
            "tell\n",
            "bit\n",
            "thing\n",
            "camp\n",
            "pretty\n",
            "decide\n",
            "water\n",
            "stay\n",
            "route\n",
            "river\n",
            "end\n",
            "new\n",
            "home\n",
            "use\n",
            "close\n",
            "work\n",
            "dinner\n",
            "old\n",
            "meet\n",
            "lunch\n",
            "try\n",
            "morning\n",
            "rest\n",
            "area\n",
            "city\n",
            "car\n",
            "walk\n",
            "rt\n",
            "tent\n",
            "need\n",
            "talk\n",
            "ask\n",
            "run\n",
            "store\n",
            "bad\n",
            "trail\n",
            "hot\n",
            "arrive\n",
            "far\n",
            "couple\n",
            "guy\n",
            "food\n",
            "small\n",
            "south\n",
            "set\n",
            "mountain\n",
            "wait\n",
            "year\n",
            "minute\n",
            "flat\n",
            "spend\n",
            "room\n",
            "sleep\n",
            "shower\n",
            "early\n",
            "campground\n",
            "traffic\n",
            "lake\n",
            "tour\n",
            "point\n",
            "sit\n",
            "north\n",
            "away\n",
            "continue\n",
            "finally\n",
            "state\n",
            "drive\n",
            "tomorrow\n",
            "cross\n",
            "house\n",
            "later\n",
            "tire\n",
            "west\n"
          ]
        }
      ],
      "source": [
        "# https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html\n",
        "dictionary = corpora.Dictionary.load(\"dictionary.dict\")\n",
        "corpus = corpora.MmCorpus(\"corpus.mm\")\n",
        "\n",
        "word_freq = {k: v for k, v in sorted(dictionary.cfs.items(), key=lambda item: item[1], reverse=True)}\n",
        "for id in list(word_freq.keys())[:100]:\n",
        "    print(dictionary[id])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "2g0kxnG38TLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import re\n",
        "\n",
        "# Allow logging\n",
        "logging.basicConfig(filename='gensim.log', format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)\n",
        "\n",
        "\n",
        "def clear_logfile():\n",
        "    # To empty the log file\n",
        "    with open(\"gensim.log\", \"w\"):\n",
        "        pass\n",
        "\n",
        "\n",
        "def parse_logfile():\n",
        "    \"\"\"Parse gensim.log to extract the log-likelihood scores.\n",
        "\n",
        "    Returns:\n",
        "        A list of log-likelihood scores.\n",
        "    \"\"\"\n",
        "    matcher = re.compile(r'(-*\\d+\\.\\d+) per-word .* (\\d+\\.\\d+) perplexity')\n",
        "    likelihoods = []\n",
        "    with open('gensim.log') as source:\n",
        "        for line in source:\n",
        "            match = matcher.search(line)\n",
        "            if match:\n",
        "                likelihoods.append(float(match.group(1)))\n",
        "    return likelihoods"
      ],
      "metadata": {
        "id": "68ZfkEG4xRka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "\n",
        "# # https://radimrehurek.com/gensim/models/ldamodel.html\n",
        "clear_logfile()\n",
        "lda = models.LdaModel(corpus_tfidf, num_topics=10)\n",
        "likelihoods = parse_logfile()\n",
        "\n",
        "# Plot likelihoods for convergence\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "ax.plot(likelihoods)\n",
        "ax.set(title=\"LDA Convergence\", xlabel=\"Iteration\", ylabel=\"Log-Likelihood\")\n",
        "\n",
        "# Print most important words per topic\n",
        "topics = lda.get_topics()\n",
        "for topic in range(10):\n",
        "    topic_probs = topics[topic, :]\n",
        "    print(f\"Topic {topic}: {', '.join([dictionary[i] for i in np.argsort(topic_probs)[-10:]])}\")"
      ],
      "metadata": {
        "id": "Ajty4TDm7fwA",
        "outputId": "ebc2b84b-31b9-490c-dd47-8bd06f5f8989",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.ldamodel:no word id mapping provided; initializing from corpus, assuming identity\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: town, wind, park, time, pass, city, climb, rain, rt, i\n",
            "Topic 1: area, route, trail, town, train, climb, time, park, total, i\n",
            "Topic 2: stop, camp, lake, pass, rain, rt, town, hill, campground, i\n",
            "Topic 3: stop, turn, night, thank, tent, hill, town, rain, rt, i\n",
            "Topic 4: town, route, park, wind, pass, height, river, little, climb, i\n",
            "Topic 5: park, hill, time, pass, campground, find, place, town, rt, i\n",
            "Topic 6: river, town, pass, stop, little, rain, head, night, total, i\n",
            "Topic 7: time, town, place, little, tent, pass, hill, stop, rt, i\n",
            "Topic 8: pretty, pj, hill, time, guy, river, tomorrow, town, rain, i\n",
            "Topic 9: climb, rain, water, rt, head, town, park, time, trail, i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n-lG-OXc8wXp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}